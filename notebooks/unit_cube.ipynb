{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x] see if we have multi query capabilities\n",
    "# [x] expose connection radius infrastructure\n",
    "# what we want:\n",
    "# [x] we want to create new states first with a specific connection radius - implement a ConnectionFilter instead of trying to import BoundedConnectionStrategy\n",
    "# [x] then we want to reconfigure the planner to only create new states when adding start and goals without growing the roadmap more (just 'using') - clear input query every time\n",
    "# [x] decide on experiment apparatus\n",
    "# Run from d: 2....20\n",
    "# Naive: Pre-constructed instance (empty, narrow corridor, etc.). Then run with m (~500>) different roadmaps. Each roadmap is a single Monte-Carlo sample.\n",
    "# Naive: Run n trials on one roadmap <-- problem: number of trials we need to run scales alongside epsilon nets. ironic! ~either sample new states or just reuse graph states? sample lots of states and reuse?\n",
    "# [x] write up experiment apparatus\n",
    "# [x] switch to manually defining problems instead of simple setup since we need more control\n",
    "# [x] define validation of ONE d-dimensional roadmap\n",
    "# [x] add in multiple d-dimensional roadmaps\n",
    "# [x] add in control of graph growth in solution criterion\n",
    "# [x] add in parameter/n prob solution infrastructure\n",
    "# AS SAMPLES INCREASE m inc.\n",
    "# maintain a list of tolerances\n",
    "# [x] add in pandas dataframe charting infrastructure\n",
    "# current array format: N_trial x M_sample x T_tol -> # frac paths rel opt\n",
    "\n",
    "# [x] implement StateValidityChecker for d-dimensional snake environments\n",
    "# [ ] implement a ValidStateSampler so we do not need to rejection-sample in high-dimensional environments\n",
    "# [ ] work out math and implement the strategic spots we need to check for full environment coverage in snake\n",
    "# [ ] save more data than just estimated success rate\n",
    "\n",
    "# [ ] modularize the pieces of the script to be easily integrated into a supercloud mapreduce pattern\n",
    "# [ ] design parameters and run experiment\n",
    "# --what is likely going to happen is that given the current level of control we have form the python bindings, since we can only query individual paths, it's going to be too slow.\n",
    "# --run anyway so we can present data to nick\n",
    "# --but for interpretability, _now_ formally prove the result that means `if we want uniform converage with \\epsilon-optimality' => we need an \\epsilon'-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T01:24:30.337573692Z",
     "start_time": "2023-08-22T01:24:30.292423377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMPL_PATH=/home/seiji/Research/ompl/py-bindings\n"
     ]
    }
   ],
   "source": [
    "%env OMPL_PATH= /home/seiji/Research/ompl/py-bindings\n",
    "from nonasymptotic import ompl\n",
    "from ompl import base as ob\n",
    "from ompl import geometric as og\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sympy.combinatorics.graycode import GrayCode\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "import json\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "class GrayCodePathWalls:\n",
    "    def __init__(self, dim, length, thickness=0.0):\n",
    "        assert dim >= 2\n",
    "        assert length > 0\n",
    "\n",
    "        gray_code = GrayCode(dim - 1)\n",
    "        gray_coords = []\n",
    "        for gray_str in gray_code.generate_gray():\n",
    "            gray_coords.append(tuple(list(map(lambda s: int(s), [*gray_str]))))\n",
    "\n",
    "        gray_coords_reversed = copy.deepcopy(gray_coords)\n",
    "        gray_coords_reversed.reverse()\n",
    "\n",
    "        # linear graph, represented as a list (NOTE: use NetworkX if we need something more powerful) \n",
    "        no_walls_linear_list = []\n",
    "        for i in range(length):\n",
    "            if i % 2 == 0:\n",
    "                no_walls_linear_list += self._prepend_coord(i, gray_coords)\n",
    "            else:\n",
    "                no_walls_linear_list += self._prepend_coord(i, gray_coords_reversed)\n",
    "\n",
    "        no_walls_edge_list = list(zip(no_walls_linear_list, no_walls_linear_list[1:]))\n",
    "\n",
    "        self.no_walls_graph = nx.Graph()\n",
    "        self.no_walls_graph.add_nodes_from(no_walls_linear_list)\n",
    "        self.no_walls_graph.add_edges_from(\n",
    "            no_walls_edge_list)  # we could store the complement, (edge = wall) but that's larger memory\n",
    "\n",
    "        self.dim = dim\n",
    "        self.length = length\n",
    "        self.thickness = thickness\n",
    "\n",
    "    def distance_to_wall(self, x):\n",
    "        assert x.shape == (self.dim,)\n",
    "\n",
    "        # begin by rounding to know which hypercube we are in \n",
    "        cube_coords = np.round(x).astype('int64')\n",
    "\n",
    "        # translate to cube center\n",
    "        leveler = np.zeros(self.dim)\n",
    "        leveler[0] += int(x[0])\n",
    "        x_c = (x - leveler) - ((cube_coords - leveler) / 2 + 0.25)\n",
    "\n",
    "        # get neighbors so we know there are free passageways\n",
    "        neighbors = list(self.no_walls_graph.neighbors(tuple(cube_coords)))\n",
    "\n",
    "        # fill in coordinates of walls, accounting for neighbors with no walls\n",
    "        walls_low = np.ones(self.dim) * -0.25\n",
    "        walls_high = np.ones(self.dim) * 0.25\n",
    "\n",
    "        for i in range(len(neighbors)):\n",
    "            walls_low, walls_high = self._unblock_wall(\n",
    "                cube_coords, np.array(neighbors[i]), walls_low, walls_high)\n",
    "\n",
    "        walls_low_dists = np.abs(x_c - walls_low)\n",
    "        walls_high_dists = np.abs(x_c - walls_high)\n",
    "\n",
    "        return min(np.min(walls_low_dists), np.min(walls_high_dists)) - self.thickness\n",
    "\n",
    "    @staticmethod\n",
    "    def _unblock_wall(cube_coords, neighbor_coords, walls_low, walls_high):\n",
    "        neighbor_diff = neighbor_coords - cube_coords\n",
    "        neighbor_diff_ind = np.where(neighbor_diff)\n",
    "        neighbor_diff_sign = neighbor_diff[neighbor_diff_ind]\n",
    "\n",
    "        if neighbor_diff_sign > 0:\n",
    "            walls_high[neighbor_diff_ind] = np.inf\n",
    "        else:\n",
    "            walls_low[neighbor_diff_ind] = -np.inf\n",
    "\n",
    "        return walls_low, walls_high\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepend_coord(coord, list_of_coords):\n",
    "        return list(\n",
    "            map(\n",
    "                lambda tup: (coord,) + tup,\n",
    "                list_of_coords\n",
    "            )\n",
    "        )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:57:52.195974754Z",
     "start_time": "2023-08-22T03:57:52.192892478Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[0.   0.25]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "env = GrayCodePathWalls(2, 5)\n",
    "print(env.distance_to_wall(np.array([0.25, 1.0])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T03:59:42.525673769Z",
     "start_time": "2023-08-22T03:59:42.522366880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T04:03:40.517732326Z",
     "start_time": "2023-06-12T04:03:40.512594022Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining free parameters: dimension, obstacles (i.e. state validity)\n",
    "sample_schedule = [10, 100, 1000]  #[10, 100, 1000, 10000, 100000]\n",
    "tol = 0.5\n",
    "delta = 0.25\n",
    "n_trials = 10\n",
    "ds = [2, 3]  #[2, 6, 10, 14, 17, 20]\n",
    "r = tol / np.sqrt(1 + tol * tol) * delta\n",
    "n_paths = 5  # should be a function of dimensionality in the future for coverage\n",
    "is_state_valid = lambda state: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T04:03:43.489729278Z",
     "start_time": "2023-06-12T04:03:43.486537120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Made that log directory already!\n"
     ]
    }
   ],
   "source": [
    "# make a new log directory and save experiment params\n",
    "name = 'long'\n",
    "date_str = datetime.now().strftime('%m-%d-%Y')\n",
    "log_dir = 'data/' + date_str + '_' + name\n",
    "try:\n",
    "    os.mkdir(log_dir)\n",
    "except FileExistsError:\n",
    "    print('WARNING: Made that log directory already!')\n",
    "\n",
    "params = {\n",
    "    \"sample_schedule\": sample_schedule,\n",
    "    \"tol\": tol,\n",
    "    \"ds\": ds,\n",
    "    \"n_paths\": n_paths,\n",
    "    \"n_trials\": n_trials,\n",
    "}\n",
    "\n",
    "with open(os.path.join(log_dir, 'params.json'), 'w') as handle:\n",
    "    json.dump(params, handle, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T04:04:07.068132670Z",
     "start_time": "2023-06-12T04:03:55.352950116Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in ds:\n",
    "    # set up unit cube space with obstacles\n",
    "    space = ob.RealVectorStateSpace(d)  # TODO: rather than construct a unit cube, construct the snake environment\n",
    "    space.setBounds(0.0, 1.0)  # we'll need to do that with a new type of state validity checker\n",
    "\n",
    "    si = ob.SpaceInformation(space)\n",
    "    si.setStateValidityChecker(ob.StateValidityCheckerFn(is_state_valid))\n",
    "    pdef = ob.ProblemDefinition(si)\n",
    "\n",
    "    # TODO: define a ValidStateSampler to sample directly from the space without having to evaluate isValid\n",
    "\n",
    "    # set up the PRM planner\n",
    "    planner = og.PRM(si)\n",
    "    accept_if_within_r = lambda v1, v2: planner.distanceFunction(v1, v2) <= r\n",
    "    planner.setConnectionFilter(og.ConnectionFilter(accept_if_within_r))\n",
    "    planner.setProblemDefinition(pdef)\n",
    "    planner.setup()  # no harm in recalling if setup already\n",
    "\n",
    "    rec = np.zeros((n_trials, len(sample_schedule)))\n",
    "\n",
    "    for i_trial in range(n_trials):\n",
    "        planner.clear()\n",
    "        pdef.clearStartStates()\n",
    "        pdef.clearGoal()\n",
    "\n",
    "        # save fraction of rel-optness\n",
    "        for i_sample, m_samples in enumerate(sample_schedule):\n",
    "            num_rel_opt_paths = 0\n",
    "            graph_has_m_samples = lambda: planner.milestoneCount() >= m_samples\n",
    "            term_cond = ob.PlannerTerminationConditionFn(graph_has_m_samples)\n",
    "            for i_path in range(n_paths):\n",
    "                # set up problem instance\n",
    "                start = ob.State(space)\n",
    "                start.random()\n",
    "\n",
    "                goal = ob.State(space)\n",
    "                goal.random()  # TODO: based on accuracy tolerance, choose starts and goals strategically based on space construction\n",
    "\n",
    "                # compute optimum (remember to turn objects into pointers!)\n",
    "                opt_len = space.distance(start(), goal())\n",
    "\n",
    "                planner.clearQuery()  # clear previous queries so we can multi-query\n",
    "                pdef.setStartAndGoalStates(start, goal)\n",
    "\n",
    "                # need to grow first to get to the # of states we want\n",
    "                planner.growRoadmap(term_cond)\n",
    "\n",
    "                # allow to create _one_ state\n",
    "                solved = planner.solve(0.001)\n",
    "\n",
    "                if str(solved) == 'Exact solution':\n",
    "                    path = pdef.getSolutionPath()\n",
    "                    length = path.length()\n",
    "                    opt_tol = (1 + tol) * opt_len\n",
    "                    within_tols = int(length < opt_tol)\n",
    "                    num_rel_opt_paths += within_tols\n",
    "\n",
    "            rec[i_trial, i_sample] = float(\n",
    "                num_rel_opt_paths) / n_paths  # TODO: save more data than just an estimated success rate\n",
    "\n",
    "    # save data\n",
    "    df = pd.DataFrame(rec, index=range(n_trials), columns=sample_schedule)\n",
    "    df.to_pickle(os.path.join(log_dir, 'prm_%s_d%i.pkl' % (date_str, d)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
