{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T15:18:20.209433748Z",
     "start_time": "2023-12-01T15:18:20.204318608Z"
    }
   },
   "outputs": [],
   "source": [
    "# [x] see if we have multi query capabilities\n",
    "# [x] expose connection radius infrastructure\n",
    "# what we want:\n",
    "# [x] we want to create new states first with a specific connection radius - implement a ConnectionFilter instead of trying to import BoundedConnectionStrategy\n",
    "# [x] then we want to reconfigure the planner to only create new states when adding start and goals without growing the roadmap more (just 'using') - clear input query every time\n",
    "# [x] decide on experiment apparatus\n",
    "# Run from d: 2....20\n",
    "# Naive: Pre-constructed instance (empty, narrow corridor, etc.). Then run with m (~500>) different roadmaps. Each roadmap is a single Monte-Carlo sample.\n",
    "# Naive: Run n trials on one roadmap <-- problem: number of trials we need to run scales alongside epsilon nets. ironic! ~either sample new states or just reuse graph states? sample lots of states and reuse?\n",
    "# [x] write up experiment apparatus\n",
    "# [x] switch to manually defining problems instead of simple setup since we need more control\n",
    "# [x] define validation of ONE d-dimensional roadmap\n",
    "# [x] add in multiple d-dimensional roadmaps\n",
    "# [x] add in control of graph growth in solution criterion\n",
    "# [x] add in parameter/n prob solution infrastructure\n",
    "# AS SAMPLES INCREASE m inc.\n",
    "# maintain a list of tolerances\n",
    "# [x] add in pandas dataframe charting infrastructure\n",
    "# current array format: N_trial x M_sample x T_tol -> # frac paths rel opt\n",
    "\n",
    "# [x] implement StateValidityChecker for d-dimensional snake environments\n",
    "# [x] implement a ValidStateSampler so we do not need to rejection-sample in high-dimensional environments\n",
    "# [x] work out math for strategic spots we need to check for full environment coverage in snake\n",
    "# [x] implement tolerance verification algorithm\n",
    "# [x] higher level utility function that does the check-advance-check-advance... pattern\n",
    "# [x] Finder of points that are the connection radius away`\n",
    "# [x] advance distance finder function in walls env\n",
    "# [x] corner finder/decide on curve parameterization -> connection to current env representation algorithm for advance range finder\n",
    "# [x] unit test for sanity (perhaps only the lower level bits, and then just to some quick cases with general PRMs in a low-D example)\n",
    "# [x] save more data than just estimated success rate\n",
    "\n",
    "# [ ] modularize the pieces of the script to be easily integrated into a supercloud mapreduce pattern\n",
    "# [ ] design parameters and run experiment\n",
    "# --what is likely going to happen is that given the current level of control we have form the python bindings, since we can only query individual paths, it's going to be too slow.\n",
    "# --run anyway so we can present data to nick\n",
    "# --but for interpretability, _now_ formally prove the result that means `if we want uniform converage with \\epsilon-optimality' => we need an \\epsilon'-net\n",
    "\n",
    "# brief notes on how PRM does a solve call given a current roadmap, and some starts and goals\n",
    "# solve():\n",
    "# -- start states are added as milestones, then added to the graph in accordance to the connection rules\n",
    "# -- goal states are added as milestones, then added to the graph in accordance to the connection rules\\\n",
    "# the roadmap is then built and solution is searched in two separate threads. we're interested in\n",
    "#       checkForSolution() to see if we can run a small part of that process\n",
    "# checkForSolution():\n",
    "# -- goals are added again as milestones if there are any that have not been added to the graph yet.\n",
    "# -- runs maybeConstructSolution()\n",
    "# -- updates a pointer, loops... until terminated externally\n",
    "# maybeConstructSolution():\n",
    "# -- checks if the start/goal are in the same component, returns false with an empty PathPtr otherwise.\n",
    "# -- checks if start/goal is pair allowed (certian pairs can be disallowed by the user), proceeds if so\n",
    "# -- constructSolution with constructSolution()\n",
    "# constructSolution():\n",
    "# -- run boost astar_search on the underlying graph object.\n",
    "\n",
    "# plan: it seems the plannerData is copies when passed in.\n",
    "# so have original planner build the roadmap, clone the plannerData and then decouple it.\n",
    "# stand up a new planner, pass in the plannerData.\n",
    "# addMilestone the start and goal state.\n",
    "# call construct path for the and then query the path information for the solution.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T15:25:40.789595230Z",
     "start_time": "2023-12-01T15:25:40.378053058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMPL_PATH=/home/seiji/Research/ompl/py-bindings\n"
     ]
    }
   ],
   "source": [
    "from nonasymptotic.envs import GrayCodeWalls\n",
    "from nonasymptotic.prm import SimplePRM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T15:25:40.795063857Z",
     "start_time": "2023-12-01T15:25:40.791826828Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining free parameters: dimension, obstacles (i.e. state validity)\n",
    "sample_schedule = [20]  #[10, 100, 1000, 10000, 100000]\n",
    "tol = 0.5\n",
    "delta = 0.25\n",
    "n_trials = 2\n",
    "ds = [2]  #[2, 6, 10, 14, 17, 20]\n",
    "r = tol / np.sqrt(1 + tol * tol) * delta\n",
    "test_seg_desc_num = 10  # should be a function of dimensionality in the future for coverage\n",
    "n_nns = 10\n",
    "machine_opener = 1e-2\n",
    "\n",
    "# make a new log directory and save experiment params\n",
    "name = 'long'\n",
    "date_str = datetime.now().strftime('%m-%d-%Y')\n",
    "log_dir = 'data/' + date_str + '_' + name\n",
    "try:\n",
    "    os.mkdir(log_dir)\n",
    "except FileExistsError:\n",
    "    print('WARNING: Made that log directory already!')\n",
    "\n",
    "params = {\n",
    "    \"sample_schedule\": sample_schedule,\n",
    "    \"tol\": tol,\n",
    "    \"ds\": ds,\n",
    "    \"n_trials\": n_trials,\n",
    "    \"test_seg_desc_num\": test_seg_desc_num,\n",
    "    'machine_opener': machine_opener,\n",
    "    'n_nns': n_nns\n",
    "}\n",
    "\n",
    "with open(os.path.join(log_dir, 'params.json'), 'w') as handle:\n",
    "    json.dump(params, handle, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-01T15:25:45.485220810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info:    PRM: Space information setup was not yet called. Calling now.\n",
      "t: 0.000000\n"
     ]
    }
   ],
   "source": [
    "for d in ds:\n",
    "    walls = GrayCodeWalls(d, 2, (1.0 - delta) / 2)\n",
    "\n",
    "    fields = ['has_completeness', 'mean_stretch', 'stdev_stretch', 'n_nodes', 'n_edges', 'info']\n",
    "    rec = {key: [] for key in fields}\n",
    "\n",
    "    for i_trial in range(n_trials):\n",
    "        prm = SimplePRM(r, walls.is_motion_valid, walls.sample_from_env, k_connection_neighbors=50)\n",
    "\n",
    "        # save fraction of rel-optness\n",
    "        for i_sample, m_samples in enumerate(sample_schedule):\n",
    "            prm.grow_to_n_samples(m_samples)\n",
    "\n",
    "            t = 0.0\n",
    "            has_completeness = True\n",
    "            len_diffs = []\n",
    "            normed_r = r / walls.get_curve_arclength()\n",
    "            while has_completeness and t < 1.0 - normed_r:\n",
    "                print('t: %f' % t)\n",
    "                # Step 1: find query points\n",
    "                basepoint = walls.arclength_to_curve_point(t)\n",
    "                testpoint = walls.arclength_to_curve_point(t + normed_r + machine_opener)  # march along\n",
    "\n",
    "                # Step 2: run PRM algorithm on basepoint and test point. (we are going r ahead _on_ the provided path)\n",
    "                sol_length, sol_path = prm.query_solution(basepoint, testpoint)\n",
    "\n",
    "                # 1: compute the stretch difference\n",
    "                # 2: find the point in the graph that the basepoint is connected to\n",
    "                stretch_diff = (1 + tol) * r - sol_length\n",
    "                # TODO: based on how we're querying the environment/scaling, r may not be the correct length of the path we are trying to approximate\n",
    "                len_diffs += [stretch_diff]\n",
    "                basepoint_nn = sol_path[0]\n",
    "                d_first_edge = np.linalg.norm(basepoint_nn - basepoint)\n",
    "\n",
    "                # Step 3: compute the advancement along the curve (but this is the point within the ball\n",
    "                # that is within the slop radius of the nearest neighbor on the graph to the test point)\n",
    "                # and yep! we need to also make sure that all the check points are in the same connected component.\n",
    "                search_rad = min(d_first_edge + stretch_diff, r)\n",
    "                for tick in range(test_seg_desc_num, -1):\n",
    "                    advance = (normed_r / test_seg_desc_num) * tick\n",
    "                    cand_point = walls.arclength_to_curve_point(t + advance)\n",
    "                    if np.linalg.norm(cand_point - basepoint_nn) <= search_rad:\n",
    "                        t += (advance + machine_opener)\n",
    "                        new_testpoint = walls.arclength_to_curve_point(t)\n",
    "                        new_start = ob.State(space)\n",
    "                        for i in range(d):\n",
    "                            new_start[i] = new_testpoint[i]\n",
    "                        new_start_vertex = master_prm.addMilestone(new_start())\n",
    "                        if not master_prm.sameComponent(new_start_vertex, start_vertex):\n",
    "                            has_completeness = False\n",
    "                            rec['info'] += ['graph covering path is not a single connected component']\n",
    "                            # test_prm.clear()\n",
    "                            break\n",
    "\n",
    "                        # test_prm.clear()\n",
    "                        continue\n",
    "\n",
    "                has_completeness = False\n",
    "                rec['info'] += ['could not find advance point']\n",
    "                # test_prm.clear()\n",
    "                break\n",
    "\n",
    "            rec['has_completeness'] += [int(has_completeness)]\n",
    "            rec['mean_stretch'] += [np.mean(len_diffs)]\n",
    "            rec['stdev_stretch'] += [np.std(len_diffs)]\n",
    "            rec['n_nodes'] += [prm.num_vertices()]\n",
    "            rec['n_edges'] += [prm.num_edges()]\n",
    "\n",
    "            # free the memory for next time. \n",
    "            prm.reset()\n",
    "\n",
    "    # save data\n",
    "    mi = pd.MultiIndex.from_product([range(n_trials), sample_schedule], names=['trial', 'm_samples'])\n",
    "    df = pd.DataFrame(rec, index=mi)\n",
    "    df.to_csv(os.path.join(log_dir, 'prm_%s_d%i.pkl' % (date_str, d)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
