{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T01:17:11.669116893Z",
     "start_time": "2023-11-23T01:17:11.668510104Z"
    }
   },
   "outputs": [],
   "source": [
    "# [x] see if we have multi query capabilities\n",
    "# [x] expose connection radius infrastructure\n",
    "# what we want:\n",
    "# [x] we want to create new states first with a specific connection radius - implement a ConnectionFilter instead of trying to import BoundedConnectionStrategy\n",
    "# [x] then we want to reconfigure the planner to only create new states when adding start and goals without growing the roadmap more (just 'using') - clear input query every time\n",
    "# [x] decide on experiment apparatus\n",
    "# Run from d: 2....20\n",
    "# Naive: Pre-constructed instance (empty, narrow corridor, etc.). Then run with m (~500>) different roadmaps. Each roadmap is a single Monte-Carlo sample.\n",
    "# Naive: Run n trials on one roadmap <-- problem: number of trials we need to run scales alongside epsilon nets. ironic! ~either sample new states or just reuse graph states? sample lots of states and reuse?\n",
    "# [x] write up experiment apparatus\n",
    "# [x] switch to manually defining problems instead of simple setup since we need more control\n",
    "# [x] define validation of ONE d-dimensional roadmap\n",
    "# [x] add in multiple d-dimensional roadmaps\n",
    "# [x] add in control of graph growth in solution criterion\n",
    "# [x] add in parameter/n prob solution infrastructure\n",
    "# AS SAMPLES INCREASE m inc.\n",
    "# maintain a list of tolerances\n",
    "# [x] add in pandas dataframe charting infrastructure\n",
    "# current array format: N_trial x M_sample x T_tol -> # frac paths rel opt\n",
    "\n",
    "# [x] implement StateValidityChecker for d-dimensional snake environments\n",
    "# [x] implement a ValidStateSampler so we do not need to rejection-sample in high-dimensional environments\n",
    "# [x] work out math for strategic spots we need to check for full environment coverage in snake\n",
    "# [ ] implement tolerance verification algorithm\n",
    "# [ ] higher level utility function that does the check-advance-check-advance... pattern\n",
    "# [ ] Finder of points that are the connection radius away`\n",
    "# [ ] advance distance finder function in walls env\n",
    "# [ ] corner finder/decide on curve parameterization -> connection to current env representation algorithm for advance range finder\n",
    "# [ ] unit test for sanity (perhaps only the lower level bits, and then just to some quick cases with general PRMs in a low-D example)\n",
    "# [ ] save more data than just estimated success rate\n",
    "\n",
    "# [ ] modularize the pieces of the script to be easily integrated into a supercloud mapreduce pattern\n",
    "# [ ] design parameters and run experiment\n",
    "# --what is likely going to happen is that given the current level of control we have form the python bindings, since we can only query individual paths, it's going to be too slow.\n",
    "# --run anyway so we can present data to nick\n",
    "# --but for interpretability, _now_ formally prove the result that means `if we want uniform converage with \\epsilon-optimality' => we need an \\epsilon'-net\n",
    "\n",
    "# brief notes on how PRM does a solve call given a current roadmap, and some starts and goals\n",
    "# solve():\n",
    "# -- start states are added as milestones, then added to the graph in accordance to the connection rules\n",
    "# -- goal states are added as milestones, then added to the graph in accordance to the connection rules\\\n",
    "# the roadmap is then built and solution is searched in two separate threads. we're interested in\n",
    "#       checkForSolution() to see if we can run a small part of that process\n",
    "# checkForSolution():\n",
    "# -- goals are added again as milestones if there are any that have not been added to the graph yet.\n",
    "# -- runs maybeConstructSolution()\n",
    "# -- updates a pointer, loops... until terminated externally\n",
    "# maybeConstructSolution():\n",
    "# -- checks if the start/goal are in the same component, returns false with an empty PathPtr otherwise.\n",
    "# -- checks if start/goal is pair allowed (certian pairs can be disallowed by the user), proceeds if so\n",
    "# -- constructSolution with constructSolution()\n",
    "# constructSolution():\n",
    "# -- run boost astar_search on the underlying graph object.\n",
    "\n",
    "# plan: it seems the plannerData is copies when passed in.\n",
    "# so have original planner build the roadmap, clone the plannerData and then decouple it.\n",
    "# stand up a new planner, pass in the plannerData.\n",
    "# addMilestone the start and goal state.\n",
    "# call construct path for the and then query the path information for the solution.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T20:35:52.584399890Z",
     "start_time": "2023-11-23T20:35:52.237066937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMPL_PATH=/home/seiji/Research/ompl/py-bindings\n"
     ]
    }
   ],
   "source": [
    "%env OMPL_PATH= /home/seiji/Research/ompl/py-bindings\n",
    "from nonasymptotic import ompl\n",
    "from ompl import base as ob\n",
    "from ompl import geometric as og\n",
    "from nonasymptotic.envs import GrayCodeWalls\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T20:35:54.130872465Z",
     "start_time": "2023-11-23T20:35:54.127156146Z"
    }
   },
   "outputs": [],
   "source": [
    "# defining free parameters: dimension, obstacles (i.e. state validity)\n",
    "sample_schedule = [10, 100, 1000]  #[10, 100, 1000, 10000, 100000]\n",
    "tol = 0.5\n",
    "delta = 0.25\n",
    "n_trials = 2 \n",
    "ds = [2]  #[2, 6, 10, 14, 17, 20]\n",
    "r = tol / np.sqrt(1 + tol * tol) * delta\n",
    "test_seg_desc_num = 10  # should be a function of dimensionality in the future for coverage\n",
    "machine_opener = 1e-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T20:35:55.255073261Z",
     "start_time": "2023-11-23T20:35:55.251412157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Made that log directory already!\n"
     ]
    }
   ],
   "source": [
    "# make a new log directory and save experiment params\n",
    "name = 'long'\n",
    "date_str = datetime.now().strftime('%m-%d-%Y')\n",
    "log_dir = 'data/' + date_str + '_' + name\n",
    "try:\n",
    "    os.mkdir(log_dir)\n",
    "except FileExistsError:\n",
    "    print('WARNING: Made that log directory already!')\n",
    "\n",
    "params = {\n",
    "    \"sample_schedule\": sample_schedule,\n",
    "    \"tol\": tol,\n",
    "    \"ds\": ds,\n",
    "    \"n_trials\": n_trials,\n",
    "    \"test_seg_desc_num\": test_seg_desc_num,\n",
    "    'machine_opener': machine_opener\n",
    "}\n",
    "\n",
    "with open(os.path.join(log_dir, 'params.json'), 'w') as handle:\n",
    "    json.dump(params, handle, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class GrayCodeEnvStateSampler(ob.ValidStateSampler):\n",
    "    def __init__(self, si, gc_env):\n",
    "        super(GrayCodeEnvStateSampler, self).__init__(si)\n",
    "        self.name = \"Gray code env dim: %i, length: %i, wall thickness: %f sampler\" \\\n",
    "                    % (gc_env.dim, gc_env.length, gc_env.thickness)\n",
    "        self.gc_env = gc_env\n",
    "\n",
    "    def sample(self, state):\n",
    "        for j, x_j in enumerate(self.gc_env.sample_from_env()):\n",
    "            state[j] = x_j\n",
    "        return True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T20:35:57.237470183Z",
     "start_time": "2023-11-23T20:35:57.229056026Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T21:16:49.747189372Z",
     "start_time": "2023-11-23T20:38:24.075341890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info:    PRM: Space information setup was not yet called. Calling now.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 72\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# TODO: give a thought to edge collision check/whether we need it\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 72\u001B[0m     sol_path \u001B[38;5;241m=\u001B[39m \u001B[43mtest_prm\u001B[49m\u001B[38;5;241m.\u001B[39mconstructSolution(start_vertex, goal_vertex)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     74\u001B[0m     rec[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minfo\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCould not find path: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m e]\n",
      "Cell \u001B[0;32mIn[14], line 72\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# TODO: give a thought to edge collision check/whether we need it\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 72\u001B[0m     sol_path \u001B[38;5;241m=\u001B[39m \u001B[43mtest_prm\u001B[49m\u001B[38;5;241m.\u001B[39mconstructSolution(start_vertex, goal_vertex)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     74\u001B[0m     rec[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minfo\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCould not find path: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m e]\n",
      "File \u001B[0;32m/snap/pycharm-professional/359/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:747\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 747\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m/snap/pycharm-professional/359/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:144\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/snap/pycharm-professional/359/plugins/python/helpers/pydev/pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/snap/pycharm-professional/359/plugins/python/helpers/pydev/pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for d in ds:\n",
    "    walls = GrayCodeWalls(d, 2, (1.0 - delta) / 2)\n",
    "\n",
    "    # set up unit cube space with obstacles\n",
    "    space = ob.RealVectorStateSpace(d)\n",
    "    space.setBounds(0.0, 1.0)  # we'll need to do that with a new type of state validity checker\n",
    "\n",
    "    si = ob.SpaceInformation(space)\n",
    "    pdef = ob.ProblemDefinition(si)\n",
    "    \n",
    "    def walls_validity_checker(s):\n",
    "        arr_s = np.zeros(d)\n",
    "        for j in range(d):\n",
    "            arr_s[j] = s[j]\n",
    "        return walls.distance_to_wall(arr_s) > 0.0\n",
    "    \n",
    "    si.setStateValidityChecker(ob.StateValidityCheckerFn(walls_validity_checker))\n",
    "    si.setValidStateSamplerAllocator(ob.ValidStateSamplerAllocator(lambda _si: GrayCodeEnvStateSampler(si, walls)))\n",
    "\n",
    "    # set up the PRM planner\n",
    "    master_prm = og.PRM(si)\n",
    "    master_prm.setProblemDefinition(pdef)\n",
    "    # TODO: make sure this is right way to set a connection radius\n",
    "    accept_if_within_r = og.ConnectionFilter(lambda v1, v2: master_prm.distanceFunction(v1, v2) <= r)\n",
    "    master_prm.setConnectionFilter(accept_if_within_r)\n",
    "    master_prm.setup()  # no harm in recalling if setup already\n",
    "\n",
    "    fields = ['has_completeness', 'mean_stretch', 'stdev_stretch', 'n_nodes', 'n_edges', 'info']\n",
    "    rec = {key: [] for key in fields}\n",
    "\n",
    "    for i_trial in range(n_trials):\n",
    "        master_prm.clear()\n",
    "\n",
    "        # save fraction of rel-optness\n",
    "        for i_sample, m_samples in enumerate(sample_schedule):\n",
    "            # TODO: we may need to properly set the connection strategy\n",
    "            graph_has_m_samples = lambda: master_prm.milestoneCount() >= m_samples\n",
    "            term_cond = ob.PlannerTerminationConditionFn(graph_has_m_samples)\n",
    "            master_prm.growRoadmap(term_cond)\n",
    "            roadmap_data = ob.PlannerData(si)\n",
    "            master_prm.getPlannerData(roadmap_data) # NOTE:since data is added by iterating over the edges, isolated nodes are not added, but that's alright.\n",
    "            roadmap_data.decoupleFromPlanner()  # just to make sure there isn't overlap.\n",
    "\n",
    "            t = 0.0\n",
    "            has_completeness = True\n",
    "            len_diffs = []\n",
    "            normed_r = r / walls.get_curve_arclength()\n",
    "            while has_completeness and t < 1.0 - normed_r:\n",
    "                # Step 1: find query points\n",
    "                basepoint = walls.arclength_to_curve_point(t)\n",
    "                testpoint = walls.arclength_to_curve_point(t + normed_r + machine_opener)  # march along\n",
    "\n",
    "                # Step 2: run PRM algorithm on basepoint and test point. (we are going r ahead _on_ the provided path)\n",
    "                # there is a good chance we will operate and search on the planning data directly... read the PRM code\n",
    "                # on how to do this.\n",
    "\n",
    "                test_prm = og.PRM(roadmap_data)\n",
    "                test_prm.setConnectionFilter(accept_if_within_r)\n",
    "                test_prm.setProblemDefinition(pdef)\n",
    "                test_prm.setup()\n",
    "\n",
    "                start = ob.State(space)\n",
    "                goal = ob.State(space)\n",
    "                for i in range(d):\n",
    "                    start[i] = basepoint[i]\n",
    "                    goal[i] = testpoint[i]\n",
    "\n",
    "                start_vertex = test_prm.addMilestone(start())  # this also does the graph connections\n",
    "                goal_vertex = test_prm.addMilestone(goal())   \n",
    "                # TODO: give a thought to edge collision check/whether we need it\n",
    "                try:\n",
    "                    sol_path = test_prm.constructSolution(start_vertex, goal_vertex)\n",
    "                except Exception as e:\n",
    "                    rec['info'] += ['Could not find path: %s' % e]\n",
    "                    has_completeness = False\n",
    "                    test_prm.freeMemory()\n",
    "                    break\n",
    "                   \n",
    "                # 1: compute the stretch difference\n",
    "                # 2: find the point in the graph that the basepoint is connected to\n",
    "                stretch_diff = (1 + tol) * r - sol_path.length()\n",
    "                len_diffs += [stretch_diff]\n",
    "                start_nn = sol_path.getState(1)\n",
    "                basepoint_nn = np.array([start_nn[i] for i in range(d)])\n",
    "                d_first_edge = test_prm.distanceFunction(start, basepoint_nn)\n",
    "\n",
    "                # Step 3: compute the advancement along the curve (but this is the point within the ball\n",
    "                # that is within the slop radius of the nearest neighbor on the graph to the test point)\n",
    "                # and yep! we need to also make sure that all the check points are in the same connected component.\n",
    "                search_rad = min(d_first_edge + stretch_diff, r)\n",
    "                for tick in range(test_seg_desc_num, -1):\n",
    "                    advance = (normed_r / test_seg_desc_num) * tick\n",
    "                    cand_point = walls.arclength_to_curve_point(t + advance)\n",
    "                    if np.linalg.norm(cand_point, basepoint_nn) <= search_rad:\n",
    "                        t += (advance + machine_opener)\n",
    "                        new_testpoint = walls.arclength_to_curve_point(t)\n",
    "                        new_start = ob.State(space)\n",
    "                        for i in range(d):\n",
    "                           new_start[i] = new_testpoint[i]\n",
    "                        new_start_vertex = test_prm.addMilestone(new_start()) \n",
    "                        if not test_prm.sameComponent(new_start_vertex, start_vertex):\n",
    "                            has_completeness = False\n",
    "                            rec['info'] += ['graph covering path is not a single connected component']\n",
    "                            test_prm.freeMemory()\n",
    "                            break\n",
    "                        \n",
    "                        test_prm.freeMemory()\n",
    "                        continue\n",
    "                        \n",
    "                has_completeness = False\n",
    "                rec['info'] += ['could not find advance point']\n",
    "                test_prm.freeMemory()\n",
    "                break\n",
    "         \n",
    "            rec['has_completeness'] += [int(has_completeness)]\n",
    "            rec['mean_stretch'] += [np.mean(len_diffs)]\n",
    "            rec['stdev_stretch'] += [np.std(len_diffs)]\n",
    "            rec['n_nodes'] += [roadmap_data.numVertices()]\n",
    "            rec['n_edges'] += [roadmap_data.numEdges()]\n",
    "            # work out how to save the data (and any other auxilary information that would be helpful for getting insight to how the planner works) \n",
    "                \n",
    "    # save data\n",
    "    mi = pd.MultiIndex.from_product([range(n_trials), sample_schedule], names=['trial', 'm_samples'])\n",
    "    df = pd.DataFrame(rec, index=mi)\n",
    "    df.to_csv(os.path.join(log_dir, 'prm_%s_d%i.pkl' % (date_str, d)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Boost.Python.class"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(og.PRM)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T00:25:59.493416183Z",
     "start_time": "2023-11-23T00:25:59.484098385Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
